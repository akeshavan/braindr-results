<template>

  <div class="text-center">

    <h1 class="container mt-3 pt-3"><div>Combining citizen science and deep learning to amplify expertise in neuroimaging</div></h1><div class="container authors">Anisha Keshavan<sup>1,2,3</sup>, Jason Yeatman<sup>1,2</sup>, Ariel Rokem<sup>2,3</sup></div> <div class="container affiliations text-muted"><sup>1</sup>University of Washington, Department of Speech and Hearing<br><sup>2</sup> University of Washington Institute for Neuroengineering<br><sup>3</sup>University of Washington eScience Institute</div> <br><div class="container abstract text-justify">Research in many fields has become increasingly reliant on large and complex datasets. “Big Data” holds untold promise to rapidly advance science by tackling new questions that cannot  be answered with smaller  datasets. While powerful, research with Big Data poses unique challenges, as many standard lab protocols rely on experts examining each one of the samples. This is not feasible for large-scale datasets because manual approaches are time-consuming and hence difficult to scale. Meanwhile, automated approaches lack the accuracy of examination by highly  trained scientists and this may introduce major errors, sources of noise, and unforeseen biases into these large and complex datasets. Our proposed solution is to 1) start with a small, expertly labelled dataset, 2) amplify labels through web-based tools that engage citizen scientists, and 3) train machine learning on amplified labels to emulate expert decision making. As a proof of concept, we developed a system to quality control a large dataset of &nbsp;three-dimensional magnetic resonance images (MRI) of human brains. An initial dataset of 200 brain images labeled by experts were amplified by citizen scientists  to label 722 brains, with over 80,000 ratings done through a simple web interface. A deep learning algorithm was then trained to predict data quality, based on a combination of the citizen scientist labels that accounts for differences in the quality of  classification by different citizen scientists. In an ROC analysis (on left out test data), the deep learning network performed as well as a state-of-the-art, specialized algorithm (MRIQC) for quality control of T1-weighted images, each with an area under the curve of 0.99.&nbsp;Finally, as a specific practical&nbsp;application of the method, we explore how brain image quality relates to the replicability of a well established relationship between brain volume and age over development. Combining citizen science and deep learning can generalize and scale expert decision making; this is particularly important in emerging disciplines where specialized, automated tools do not already exist.</div><div class="container content text-justify mt-3 mb-3"></div><h1 data-label="712766" class="ltx_title_section container text-justify">Introduction</h1><div class="container content text-justify mt-3 mb-3">Many research fields ranging from astronomy, to genomics, to neuroscience are entering an era of Big Data. Large and complex datasets promise to address many scientific questions, but they also present a new set of challenges. For example, over the last few years human neuroscience&nbsp;has evolved into a Big Data field. In the past, individual groups would each collect their own samples of data from a relatively small group of individuals. More recently, large data sets collected from many thousands of individuals are increasingly more common. This transition has been facilitated through &nbsp;assembly of large aggregated datasets, containing measurements from many individuals, and collected through consortium efforts such as the Human Connectome Project&nbsp;<cite class="ltx_cite raw v1"> (<a href="#glasser2016human">Glasser, 2016</a>)</cite>. These efforts, and the large datasets that they are assembling, promise to enhance our understanding&nbsp; of the relationship between brain anatomy, brain activity and cognition. The field is experiencing a paradigm shift&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Fan_2014">Fan, 2014</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4236847&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, where our once established scientific procedures are morphing as dictated by the new challenges posed by large datasets.&nbsp;We’ve seen a shift from desktop computers to cyberinfrastructure&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Van_Horn_2013">Van Horn, 2013</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3983169&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, from small studies siloed in individual labs to an explosion of data sharing initiatives&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Ferguson_2014">Ferguson, 2014</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4728080&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>, <a href="#Poldrack_2014">Poldrack, 2014</a>
            <a href="http://pubman.mpdl.mpg.de/pubman/faces/viewItemOverviewPage.jsp?itemId=escidoc:2075310" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>,&nbsp;<b>&nbsp;</b>from idiosyncratic data organization and analysis scripts to standardized file structures and workflows&nbsp;<cite class="ltx_cite raw v1"> (<a href="#gorgolewski2016brain">Gorgolewski, 2016</a>, <a href="#gorgolewski2017bids">Gorgolewski, 2017</a>)</cite>,<b>&nbsp;</b>and an overall shift in statistical thinking and computational methods&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Fan_2014">Fan, 2014</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4236847&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> that can accommodate large datasets.&nbsp;But one often overlooked aspect of our protocols in neuroimaging has not yet evolved to the needs of Big Data: expert decision making.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">Specifically, decisions made by scientists with expertise in neuroanatomy and MRI methods (i.e., neuroimaging experts) through visual inspection of imaging data cannot be accurately scaled to large datasets.&nbsp;For example, when inspecting an MRI image of the brain, there is extensive variation in neuroanatomy across individuals, and variation in image acquisition and imaging artifacts; knowing which of these variations are acceptable versus abnormal comes with years of training and experience. Specific research questions require even more training and domain expertise in a particular method, such as tracing &nbsp;anatomical regions of interest (ROIs), editing fascicle models from streamline tractography &nbsp;<cite class="ltx_cite raw v1"> (<a href="#Jordan_2017">Jordan, 2017</a>
            <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/jon.12467" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, evaluating cross-modality image alignment, and quality control of images at each stage of image processing.&nbsp;On large datasets, especially longitudinal multisite consortium studies, these expert decisions cannot be reliably replicated because the timeframe of these studies is long, individual experts get fatigued, and training teams of experts is time consuming, difficult and costly. As datasets grow to hundreds of thousands of brains it is no longer feasible to depend on manual interventions.</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">One solution to this problem is to train machines to emulate expert decisions. However, there are many cases in which automated algorithms exist, but expert decision-making is still required for optimal results. For example, a variety of image segmentation algorithms have been developed to replace manual ROI editing, with  Freesurfer&nbsp;<cite class="ltx_cite raw v1"> (<a href="#fischl2012freesurfer">Fischl, 2012</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3685476&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, FSL&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Patenaude_2011">Patenaude, 2011</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3417233&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, ANTS&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Avants_2011">Avants, 2011</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3065962&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, and SPM&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Ashburner_2005">Ashburner, 2005</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4809890&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>  all offering automated segmentation tools for standard brain structures. But these algorithms were developed on a specific type of image (T1-weighted) and on a specific type of brain (those of healthy controls). Pathological brains, or those of children or the elderly may violate the assumptions of these algorithms, and their outputs often still require manual expert editing. Similarly, in tractography, a set of anatomical ROIs can be used to target or constrain streamlines to automatically extract fascicles of interest&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#CATANI_2008">Catani, 2008</a>
            <a href="http://www.hal.inserm.fr/inserm-00322865" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>, <a href="#yeatman2012tract">Yeatman, 2012</a>
            <a href="http://europepmc.org/articles/PMC3498174?pdf=render" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>.&nbsp;But again, abnormal brain morphology resulting from pathology would still require expert editing&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Jordan_2017a">Jordan, 2017</a>
            <a href="https://www.biorxiv.org/content/biorxiv/early/2017/05/20/140137.full.pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>.&nbsp; The delineation of retinotopic maps in visual cortex is another task that has been recently automated&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Benson2014">Benson, 2014</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3967932&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>, <a href="#Benson2012">Benson, 2012</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3494819&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, but these procedures are limited to only a few of the known retinotopic maps and substantial expertise is still required to delineate the other known maps&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Winawer2017">Winawer, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5687318&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>, <a href="#Wandell2011">Wandell, 2011</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3030662&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. Another fundamental step in brain image processing that still requires expert examination is quality control. There are several automated methods to quantify image quality, based on MRI physics and the statistical properties of images, and these methods have been collected under one umbrella in an algorithm called MRIQC&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. However, these methods are specific to T1-weighted images, and cannot generalize to different image acquisition methods. To address all of these cases, and scale to new, unforeseen challenges, we need a general-purpose framework that can train machines to emulate experts for any purpose, allowing scientists to fully realize the potential of Big Data.</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">One general solution that is rapidly gaining traction is deep learning. Specifically, convolutional neural networks (CNNs) have shown promise in a variety of biomedical image processing tasks. Modeled loosely on the human visual system, CNNs can be trained for a variety of image classification and segmentation tasks using the same architecture. For example, the U-Net&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#ronneberger2015u">Ronneberger, 2015</a>)</cite>&nbsp;which was originally built for segmentation of neurons in electron microscope images, has also been adapted to segment macular edema in optical coherence tomography images&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Lee_2017">Lee, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5508840&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, to segment breast and fibroglandular tissue&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Dalm__2017">Dalmış, 2017</a>
            <a href="http://repository.ubn.ru.nl/handle/2066/173062" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, and a 3D adaptation was developed to segment the&nbsp;Xenopus kidney&nbsp;<cite class="ltx_cite raw v1"> (<a href="#cciccek20163d">Çiçek, 2016</a>)</cite>.&nbsp;Transfer learning is another broadly applicable deep learning technique, where a number of layers from pretrained network are retrained for a different use case. This can drastically cut down the training time and labelled dataset size needed&nbsp;<cite class="ltx_cite raw v1"> (<a href="#ahmed2008training">Ahmed, 2008</a>, <a href="#pan2010survey">Jialin, 2010</a>
            <a href="https://ieeexplore.ieee.org/document/5288526/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. For example, the same transfer learning approach was used for brain MRI tissue segmentation (gray matter, white matter, and CSF) and for multiple sclerosis lesion segmentation&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#van2015transfer">Annegreet, 2015</a>
            <a href="https://curis.ku.dk/portal/da/publications/transfer-learning-improves-supervised-image-segmentation-across-imaging-protocols(07623de4-ed6c-4d29-867a-ea9a1a639dba).html" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. Yet despite these&nbsp;advances in deep learning, there is one major constraint to generalizing these methods to new imaging problems: a large amount of labelled data is still required to train CNNs. Thus, even with the cutting-edge machine learning methods available, researchers seeking to automate these processes are still confronted with the original problem: how does a single expert create an annotated dataset that is large enough to train an algorithm to automate their expertise through machine learning?</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">We propose that citizen scientists are a solution. Specifically, we hypothesize that citizen scientists  can learn from, and amplify expert decisions, to the extent where deep learning approaches become feasible. Rather than labelling hundreds or thousands of training images, an expert can employ citizen scientists to help with this task, and machine learning can identify which citizen scientists provide expert-quality data. As a proof of concept, we apply this approach to brain MRI quality control (QC): a binary classification task where images are labelled “pass” or “fail” based on image quality.&nbsp;QC is a paradigmatic example of the problem of scaling expertise, because a large degree of subjectivity still remains in QC. Each researcher has their own standards as to which images pass or fail on inspection, and this variability may have problematic effects on downstream analyses, especially statistical inference.&nbsp; Effect size estimates may depend on the input data to a statistical model. Varying QC criteria will add more uncertainty to these estimates, and might result in replication failures. For example, in&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#ducharme2016trajectories">Ducharme, 2016</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4691414&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, the authors found that QC had a significant impact on their&nbsp; estimates of&nbsp; the trajectory of cortical thickness during development. They concluded that post-processing QC (in the form of expert visual inspection) is crucial for such studies, especially due to motion artifacts in younger children. While this was feasible in their &nbsp;study of 398 subjects, this would not be possible for larger scale studies like the ABCD study, which aims to collect data on 10,000 subjects longitudinally <cite class="ltx_cite raw v1"> (<a href="#casey2018adolescent">Casey, 2018</a>)</cite>. It is therefore essential that we develop systems that can accurately emulate expert decisions, and that these systems are made openly available for the scientific community.</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">To demonstrate how citizen science and deep learning can be combined to amplify expertise in neuroimaging, we developed a citizen-science amplification and CNN procedure for the openly available Healthy Brain Network dataset (HBN;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#alexander2017open">Alexander, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5735921&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>). The HBN initiative aims to collect and publicly release data on 10,000 children over the next 6 years to facilitate the study of brain development and mental health through transdiagnostic research. The rich dataset includes MRI brain scans, EEG and eye tracking recordings, extensive behavioral testing, genetic sampling, and voice and actigraphy recordings. In order to understand the relationship between brain structure (based on MRI) and behavior (EEG, eye tracking, voice, actigraphy, behavioral data), or the association between genetics and brain structure, researchers require high quality MRI data.&nbsp;</div><div class="container content text-justify mt-3 mb-3"><b>&nbsp;</b></div><div class="container content text-justify mt-3 mb-3">In this study, we crowd-amplify image quality ratings and train a CNN on the first and second data releases of the HBN (n=722), which can be used to infer data quality on future data releases. We also demonstrate how choice of QC threshold is related to the effect size estimate on the established association between age and brain tissue volumes during development&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Lebel2011">Lebel, 2011</a>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004821/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. Finally, we show that our approach of deep learning trained on a crowd-amplified dataset matches state-of-the-art software built specifically for image QC&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. We conclude that this novel method of crowd-amplification has broad applicability across scientific domains where manual inspection by experts is still the gold-standard.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><h1 data-label="904357" class="ltx_title_section container text-justify">Results</h1><h2 data-label="499743" class="ltx_title_subsection container text-justify">Overview</h2><div class="container content text-justify mt-3 mb-3">Our primary goals were &nbsp;to 1) amplify a small, expertly labelled dataset through citizen science, 2) train a model that optimally combines citizen scientist ratings to emulate an expert, 3) train a CNN on the amplified labels, and 4) evaluate its performance on a validation dataset. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig1">1</a></span> shows an overview of the procedure and provides a summary of our results. At the outset, a group of neuroimaging experts created a gold-standard quality control dataset on a small subset of the data (n=200), through extensive visual examination of the full 3D volumes of the data. In parallel, citizen scientists were asked to “pass” or “fail” two-dimensional axial slices from the full dataset (n=722) through a web application called braindr that could be accessed through a desktop, tablet or mobile phone (<a href="https://braindr.us">https://braindr.us</a>). Amplified labels, that range from 0 (fail) to 1 (pass), were generated from citizen scientist ratings. A receiver operating characteristic (ROC) curve was generated for both the ratings averaged across citizen scientists and labels generated by fitting a classifier that weights ratings more heavily for citizen scientists who more closely matched the experts in the subset rated by both (gold-standard). &nbsp;Next, a neural network was trained to predict the weighted labels. The AUC for the predicted labels on a left out dataset was 0.99.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><img class="container mt-3 fig text-center" src="../assets/braindrOverview.svg" id="fig1"/> 
        <div class="container figcapt"><b>Figure 1</b></div>
        <div class="container text-justify mb-3"><div data-label="567433">&nbsp;<b>Overview and results of our procedure:&nbsp;</b>First, the HBN data set was rated by 4 neuroimaging experts to create a gold standard subset of data. Next, the 3D MRI scans were converted into 2D axial brain slices, which were loaded onto braindr (<a href="https://braindr.us">https://braindr.us</a>), a web application to crowdsource the quality ratings (see Methods). Area under the curve of a the Receiver Operating Characteristic curve (AUC) was calculated for the average citizen scientist quality rating for each slice. Compared to an expert-labeled test set, this resulted in an AUC of 0.95. In an effort to remove unreliable citizen scientists, the ratings were aggregated by fitting a model that weights each citizen scientist contribution to the slice score by how much that individual’s scores match those of the experts. &nbsp;The resulting AUC was  0.97. Finally, the 2D brain slices together with the weighted citizen scientist  ratings were used to train a neural  network. In an ROC analysis on left out data, the AUC of these predictions was 0.99.&nbsp;</div></div><div class="container content text-justify mt-3 mb-3">&nbsp;</div><h2 data-label="933181" class="ltx_title_subsection container text-justify">Aggregating Citizen Scientist Ratings to Emulate  Expert Labels</h2><div class="container content text-justify mt-3 mb-3">Citizen scientists who rated images through the braindr web application differed substantially in terms of how well their ratings matched the experts’ ratings on the gold-standard subset: while some provided high-quality ratings that agree with the experts most of the time, others displayed variable and unreliable ratings. In order to capitalize on citizen scientists to amplify expert ratings to new data,  a weighting of each citizen scientist was learned based on a reliable match to expert agreement in slices from the gold-standard set. We used the XGBoost algorithm&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Chen2016">Chen, 2016</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5855321&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, an ensemble method that combines a set of weak learners (decision trees) to fit the gold-standard labels based on a set of features. In our case, the features were the average rating of the slice image from each citizen scientist (some images were viewed and rated more than once, so image ratings could vary between 1=always “pass” and 0=always “fail”). We then used the weights to combine the ratings of the citizen scientists and predict the left out test set. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>A shows ROC curves of classification on the left-out test set for different training set sizes, compared to the ROC curve of a baseline model in which equal weights were assigned to each citizen scientist. We see an improvement in the AUC of the XGBoosted labels (0.97) compared to the AUC of the equi-weighted &nbsp;labels (0.95). Using the model trained on two-thirds of the gold standard data (n=670 slices), we extracted the probability scores of the classifier on all slices (see Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>B). The distribution of probability scores in Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>B matches our expectations of the data; a bimodal distribution with peaks at 0 and 1, reflecting that images are mostly perceived as “passing” or “failing” . The XGBoost model also calculates a feature importance score (F). F is the number of times that a feature (in our case, an individual citizen scientist) has split the branches of a tree, summed over all boosted trees. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>C shows the feature importance for each citizen scientist, and&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>D shows the  relationship between a citizen scientist’s importance compared to the number of images they rated. In general, the more images a citizen scientist  rates, the more important they are to the model. However, there are still exceptions where a citizen scientist rated many images and their ratings were incorrect or unreliable, so the model gave  them less weight during aggregation.&nbsp;</div><img class="container mt-3 fig text-center" src="../assets/braindr_xgboost_agg1.png" id="fig2"/> 
        <div class="container figcapt"><b>Figure 2</b></div>
        <div class="container text-justify mb-3"><div data-label="468392"><b>Braindr rating aggregation and citizen scientist importance:&nbsp;</b>A. ROC curves on the test set for various training set sizes (here n denotes the number of training slices used). The dashed line is the ROC curve of the average citizen scientist ratings for all slices. B. The distribution of XGBoost probability scores on all Braindr slices. C. Feature importance for each anonymized user. D. Relationship between citizen scientist importance and total number of ratings in the gold-standard dataset.&nbsp;</div></div><h2 data-label="152075" class="ltx_title_subsection container text-justify">Training Deep Learning to Automate Image Labeling</h2><div class="container content text-justify mt-3 mb-3">Citizen scientists accurately amplify expert ratings but, ideally, we would have a fully automated approach that can be applied to new data as it becomes available. Thus, we trained a deep learning model to predict the XGBoosted labels that were based on aggregated citizen scientist ratings.&nbsp;A VGG16 neural network&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#simonyan2014very">Simonyan, 2014</a>)</cite> pretrained on the ImageNet challenge dataset &nbsp;<cite class="ltx_cite raw v1"> (<a href="#ILSVRC15">Russakovsky, 2015</a>
            <a href="http://dspace.mit.edu/bitstream/handle/1721.1/104944/11263_2015_Article_816.pdf?sequence=1" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> was used: we removed the top layer of the network, and then trained&nbsp; a final fully-connected layer followed by a single node output layer. The training of the final layer was run for 50 epochs and the best model on the validation set was saved. To estimate the variability of &nbsp;training,  the model was separately trained through 10 different training courses, each time with a different random initialization seed. Typically, training and validation loss scores were equal at around 10 epochs, after which the model usually began to overfit (training error decreased, while validation error increased, see Figure&nbsp;<span class="au-ref raw v1"><a href="#fig3">3</a></span>A). In each of the 10 training courses, we used the model with the lowest validation error  for inference on the held out test set, and calculated the ROC AUC. AUC may be a problematic statistic when the test-set is imbalanced&nbsp;<cite class="ltx_cite raw v1"> (<a href="#saito2015precision">Saito, 2015</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4349800&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, but in this case, the test-set is almost perfectly balanced (see Methods). Thus, we found that a deep learning network trained on citizen scientist generated labels was a better match to expert ratings than citizen scientist generated labels alone: the deep learning model had an &nbsp;AUC of 0.99 (+/- standard deviation of 0.12, see Figure <span class="au-ref raw v1"><a href="#fig3">3</a></span>B).&nbsp;</div><img class="container mt-3 fig text-center" src="../assets/vgg16_results1.png" id="fig3"/> 
        <div class="container figcapt"><b>Figure 3</b></div>
        <div class="container text-justify mb-3"><div data-label="258547"><b>Deep learning training and evaluation on the left out test set:&nbsp;</b>Part A shows the training and validation loss scores for 10 training runs, each with a different initialization seed. The training loss tends towards 0 but the validation loss plateaus between 0.05 and 0.07 mean squared error at the 10th epoch. Part B shows the ROC curve of the prediction on the test set against the binary classified gold-standard slices, along with the ROC curves computed from previous analysis (the average citizen scientist rating, and the XGBoosted ratings).<b>&nbsp;&nbsp;</b></div></div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="652109" class="ltx_title_subsection container text-justify">Crowd amplification and deep learning strategy performs as well as a specialized QC algorithm</h2><div class="container content text-justify mt-3 mb-3">We validated our generalized approach of crowd-amplification and deep learning by comparing classification results against an existing, specialized algorithm for QC of T1 weighted images, called MRIQC&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. The features extracted by MRIQC are guided by the physics of  MR image acquisition and&nbsp;by the statistical properties of&nbsp;images. An XGBoost model was trained on the features extracted by MRIQC on a training subset of gold-standard images, and evaluated on a previously unseen test subset. The AUC was also 0.99, matching the performance of our crowd-trained deep learning model.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="298927" class="ltx_title_subsection container text-justify">Braindr-based quality control has a substantial impact on effect size estimates&nbsp;</h2><div class="container content text-justify mt-3 mb-3">The secondary goal of this study was to investigate &nbsp;how scaling expertise through citizen science amplification affects scientific inferences from these data. For this proof of concept, we studied brain development, which is the primary focus on the HBN dataset. Lebel and colleagues &nbsp;<cite class="ltx_cite raw v1"> (<a href="#Lebel2011">Lebel, 2011</a>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004821/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> found that increases in white matter volume and decreases in gray matter volume are roughly equal in magnitude, resulting in no overall brain volume change over development in late childhood. Based on Figure 2 in the Lebel manuscript&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Lebel2011">Lebel, 2011</a>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004821/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, we estimate an effect of&nbsp; approximately -4.3 cm<sup>3</sup>&nbsp;per year - a decrease in gray matter volume over the ages measured (See Figure 2 in the the original manuscript; we estimate the high point to be 710 cm<sup>3</sup> and the low point to be 580 cm<sup>3</sup> with a range of ages of approximately 5 years to 35 years and hence: (710-580)/(5-35) = -4.3 cm<sup>3</sup>/year). To reproduce their analysis and assess the effect of using the CNN-derived quality control estimates, we estimated gray and white matter volume in the subjects that had been scored for quality using our algorithm. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig4">4</a></span> shows gray matter volume as a function of age. Two conditions are compared: in one &nbsp;(Figure&nbsp;<span class="au-ref raw v1"><a href="#fig4">4</a></span>A) all of the subjects are included, while in the other only subjects that were passed by the CNN are included (Figure&nbsp;<span class="au-ref raw v1"><a href="#fig4">4</a></span>B, blue points). Depending on the threshold chosen, the effect of gray matter volume over age varies from -2.6 cm<sup>3</sup>/year (with no threshold) to -5.3 cm<sup>3</sup>/year (with Braindr rating &gt; 0.9). A threshold of 0.7 of either Braindr or MRIQC&nbsp;results in an effect size around -4.3 cm<sup>3</sup>&nbsp;per year, replicating the results of&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Lebel2011">Lebel, 2011</a>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004821/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>.&nbsp;A supplemental interactive version of this figure allows readers to threshold data points based on QC scores from the predicted labels of the CNN (called “Braindr ratings”), or on MRIQC XGBoost probabilities (called “MRIQC ratings”) is available at <a href="http://results.braindr.us">http://results.braindr.us</a>. Thus, quality control has a substantial impact on estimates of brain development and allowing poor quality data into the statistical model can almost entirely obscure developmental changes in gray matter volume.</div><div class="container content text-justify mt-3 mb-3"></div><HelloWorld></HelloWorld>
        <div class="container figcapt"><b>Figure 4</b></div>
        <div class="container text-justify mb-3">
<div class="interactive-caption">

<b>Impact of quality control on effect size estimates:</b> 

Results of quality control on the inferred association between gray matter volume and age during development. 

On the left-hand side, there are three histogram charts of ratings from different models. The braindr ratings (top left) are the 
predicted braindr ratings for each image from the CNN that was trained on aggregate citizen scientist labels. A 0 is considered "fail" and a 1 is considered a "pass". 

The Mindcontrol ratings (middle left) are the average Mindcontrol ratings of expert raters on the 
<a href="https://mindcontrol-hbn.herokuapp.com">Mindcontrol application</a>. 
A -5 is considered a "fail" with high confidence, and a 5 is considered a "pass" with high confidence.

The MRIQC ratings (bottom left) are predicted probabilities of the trained 
XGBoost model in classifying the gold standard images. A 0 is considered "fail" and a 1 is considered a "pass". 

Click and drag on any histogram to threshold the points on the scatter chart (middle), and run an ordinary least squares (OLS) 
model for the selected variable (gray_matter, white_matter, CSF, or whole_brain) against age. 

When data is thresholded by the deep learning model’s predicted braindr rating at 0.7, 
the effect size of gray matter volume over development 
nearly doubles when QC scores are taken into account.

</div>
</div><h1 data-label="408498" class="ltx_title_section container text-justify">Discussion</h1><div class="container content text-justify mt-3 mb-3">We have developed a system to scale expertise in neuroimaging to meet the demands of Big Data. The system uses citizen scientists to amplify an initially-small, expert-labeled dataset.&nbsp; Combined with deep learning (via CNNs), the system can then accurately perform image analysis tasks that require expertise, such as quality control (QC). We have validated our method against MRIQC, a specialized tool that was designed specifically for this use case based on knowledge of the physics underlying the signal generation process in T1-weighted images&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>. Unlike MRIQC, our method is able to generalize beyond quality control of T1-weighted images; any image-based binary classification task can be loaded onto the Braindr platform, and crowdsourced via the web. For this use-case, we demonstrated the importance of scaling QC expertise by showing how replication of a previously established results depends on a researcher’s decision on data quality. Lebel and colleagues&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Lebel2011">Lebel, 2011</a>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004821/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> report changes in gray matter volume over development&nbsp;and we find that we only replicate these findings when using a stringent quality control threshold for the input data. &nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="240928" class="ltx_title_subsection container text-justify">The Internet and Web Applications for Collaboration&nbsp;</h2><div class="container content text-justify mt-3 mb-3">The internet and web browser technologies are not only crucial for scientific communication, but also for collaboration and distribution of work. This is particularly true in the age of large consortium efforts aimed at generating high-quality large data sets. Recent progress in citizen science projects for neuroscience research have proven extremely useful and popular, in part due to the ubiquity of the web browser. Large-scale citizen science projects, like EyeWire&nbsp;<cite class="ltx_cite raw v1"> (<a href="#kim2014space">Kim, 2014</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4074887&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>, <a href="#marx2013neuroscience">Marx, 2013</a>)</cite>&nbsp;,&nbsp;and Mozak&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#roskams2016power">Roskams, 2016</a>
            <a href="https://linkinghub.elsevier.com/retrieve/pii/S0896627316001689" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>,&nbsp;have enabled scientists working with high resolution microscopy data to map neuronal connections at the microscale, with help from over 100,000 citizen scientists. In MR imaging,&nbsp; web-based tools such as&nbsp;BrainBox&nbsp;<cite class="ltx_cite raw v1"> (<a href="#heuer2016open">Heuer, 2016</a>
            <a href="https://riojournal.com/articles.php?journal_name=rio&id=9113" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>&nbsp;and Mindcontrol&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Keshavan2017">Keshavan, 2017</a>
            <a href="https://linkinghub.elsevier.com/retrieve/pii/S1053811917302707" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>&nbsp;were built to facilitate the collaboration of neuroimaging experts in image segmentation and quality control. However, the task of inspecting each slice of a 3D image in either BrainBox or Mindcontrol takes a long time, and this complex task tends to lose potential citizen scientists who find it too difficult or time consuming.&nbsp; In general, crowdsourcing is most effective when a project is broken down into short, simple, well-defined “micro-tasks”, that can be completed in short bursts of work and are resilient to interruption&nbsp;<cite class="ltx_cite raw v1"> (<a href="#cheng2015break">Cheng, 2015</a>)</cite>.<b>&nbsp;</b>&nbsp;In order to simplify the task for citizen scientists, we developed a web application called braindr, which reduces the time-consuming task of slice-by-slice 3D inspection to a quick binary choice made on a 2D slice. While we might worry that distilling a complex decision into a simple swipe on a smartphone might add noise, we demonstrated that a model could be constructed to accurately combine ratings from many citizen scientists to almost perfectly emulate those obtained from inspection by experts. Using braindr, citizen scientists amplified the initial expert-labelled dataset (200 3D images) to the entire dataset (&gt; 700 3D images, &gt; 3000 2D slices) in a few weeks. Because braindr is a lightweight web application, users could play it at any time and on any device, and this meant we were able to attract many users. On braindr, each slice received on average 20 ratings, and therefore each 3D brain (consisting of 5 slices) received on average&nbsp;100 ratings. In short, by redesigning the way we interact with our data and presenting it in the web browser, we were able to get many more eyes on our data than would have been possible in a single research lab.</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="578355" class="ltx_title_subsection container text-justify">Scaling expertise through interactions between experts, citizen scientists and machine learning</h2><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">We found that  an interaction between experts, citizen scientists, and machine learning results in scalable decision-making&nbsp;on brain MRI images. Recent advances in machine learning have vastly improved image  classification<cite class="ltx_cite raw v1"> (<a href="#krizhevsky2012imagenet">Krizhevsky, 2012</a>
            <a href="http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, object detection<cite class="ltx_cite raw v1"> (<a href="#girshick2014rich">Girshick, 2014</a>
            <a href="http://www.computeroptics.ru/jour/article/view/439" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, and segmentation<cite class="ltx_cite raw v1"> (<a href="#long2015fully">Long, 2015</a>
            <a href="https://ieeexplore.ieee.org/document/7478072/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>  through the use of deep convolutional neural networks. In the biomedical domain, these networks have been trained to accurately diagnose eye disease&nbsp;<cite class="ltx_cite raw v1"> (<a href="#lee2017deep">Lee, 2017</a>
            <a href="https://linkinghub.elsevier.com/retrieve/pii/S2468653016301749" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, diagnose skin cancer&nbsp;<cite class="ltx_cite raw v1"> (<a href="#esteva2017dermatologist">Esteva, 2017</a>)</cite>, and breast cancer&nbsp;<cite class="ltx_cite raw v1"> (<a href="#sahiner1996classification">Sahiner, 1996</a>)</cite>, to name a few applications. But these applications require a large and accurately labeled dataset. This presents an impediment for many scientific disciplines, where labeled data may be more scarce, or hard to come by, because it requires labor-intensive procedures. The approach presented here solves this fundamental bottleneck in the current application of modern machine learning approaches, and enables scientists to automate complex tasks that require substantial expertise.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">A surprising finding that emerges from this work is that a deep learning algorithm can learn to match or even exceed the aggregated ratings that are used for training. This finding is likely to reflect the fact that algorithms are more reliable than humans, and when an algorithm is trained to match human accuracy, it has the added benefit of perfect reliability. For example even an expert might not provide the exact same ratings each time they see the same image, while an algorithm will. This is in line with findings from&nbsp;<cite class="ltx_cite raw v1"> (<a href="#lee2017deepa">Lee, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5508840&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, showing that the agreement between an algorithm and any one expert can be equivalent to agreement between any pair of experts. &nbsp;We have demonstrated that while an individual citizen scientists may not provide reliable results, by intelligently combining a crowd with machine learning, and keeping an expert in the loop to monitor results, decisions can be accurately scaled to meet the demands of Big Data.</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="638082" class="ltx_title_subsection container text-justify">MRI Quality Control and Morphometrics over Development</h2><div class="container content text-justify mt-3 mb-3">The specific use-case that we focused on pertains to the importance of quality control in large-scale MRI data acquisitions. Recently, Ducharme and colleagues&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#ducharme2016trajectories">Ducharme, 2016</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4691414&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> stressed the importance of quality control for  studies of brain development in a large cohort of 954 subjects. They estimated cortical thickness on each point of a cortical surface and fit linear, quadratic and cubic models of thickness versus age at each vertex. Quality control was performed by visual inspection of the reconstructed cortical surface, and removing data that failed QC from the analysis. Without stringent quality control, the best fit models were more complex (quadratic/cubic), and with quality control the best fit models were linear. They found sex differences only at the occipital regions, which thinned faster in males. In the supplemental  figure that accompanies Figure&nbsp;<span class="au-ref raw v1"><a href="#fig4">4</a></span>, we presented an interactive chart where users can similarly explore different ordinary least squares models (linear or quadratic) and also split by sex for the relationship between total gray matter volume, white matter volume, CSF volume, and total brain volume over age.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">We chose to QC raw MRI data in this study, rather than the processed data because the quality of the raw MRI data affects the downstream cortical mesh generation, and many other computed metrics. A large body of research in automated QC of T1-weighted&nbsp;images exists, in part because of large open data sharing initiatives.&nbsp; In 2009, Mortamet and colleagues&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#mortamet2009automatic">Mortamet, 2009</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2780021&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> developed a QC algorithm based on the background of magnitude images of the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset, and reported a sensitivity and specificity of &gt; 85%. In 2015, Shezad and colleagues&nbsp;<cite class="ltx_cite raw v1"> (<a href="#shehzadpreprocessed">Shehzad, 2015</a>
            <a href="https://www.frontiersin.org/10.3389/conf.fnins.2015.91.00047/event_abstract" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>&nbsp;developed the Preprocessed Connectomes Project Quality Assessment Protocol (PCP-QAP)&nbsp; on the Autism Brain Imaging Data Exchange (ABIDE) and Consortium for Reproducibility and Reliability (CoRR) datasets. The PCP-QAP also included a Python library to easily compute metrics such as signal to noise ratio, contrast to noise ratio, entropy focus criterion, foreground-to-background energy ratio, voxel smoothness, and percentage of artifact voxels. Building on this work, the MRIQC package from Esteban and colleagues&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> includes a comprehensive set of 64 image quality metrics, from which a classifier was trained to predict data quality of the ABIDE dataset for new, unseen sites with 76% accuracy.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">Our strategy differed from that of the MRIQC classification study. In the Esteban 2017 study&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, the authors labelled images that were “doubtful” in quality as a “pass” when training and evaluating their classifier. Our MRIQC classifier was trained and evaluated only on images that our raters&nbsp;very confidently passed or failed. Because quality control is subjective, we felt that it was acceptable for a “doubtful” image to be failed by the classifier. Since our classifier was trained on data acquired within a single site, and only on images that we were confident about, our MRIQC classifier achieved near perfect accuracy with an AUC of 0.99. On the other hand, our braindr CNN was trained as a regression (rather than a classification) on the full dataset, including the “doubtful” images (i.e those with ratings closer to 0.5), but was still evaluated as a classifier against data we were confident about. This also achieved near-perfect accuracy with an AUC of 0.99. Because both the MRIQC and braindr classifiers perform so well on data we are confident about, we contend that it is acceptable to let the classifier act as a “tie-breaker” for images that lie in the middle of the spectrum, for future acquisitions of the HBN dataset.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">Quality control of large consortium datasets, and more generally, the scaling of expertise in neuroimaging, will become increasingly important as neuroscience moves towards data-driven discovery. Interdisciplinary collaboration between domain experts and computer scientists, and public outreach and engagement of citizen scientists can help realize the full potential of Big Data.</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="679893" class="ltx_title_subsection container text-justify">Limitations</h2><div class="container content text-justify mt-3 mb-3">One limitation of this method is that there is an interpretability to speed tradeoff. Specialized QC tools were developed over many years, while this study was performed in a fraction of that time. Specialized QC tools are far more interpretable; for example, the coefficient of joint variation (CJV) metric from MRIQC is sensitive to the presence of head motion. CJV was one of the most important features of our MRIQC classifier, implying that our citizen scientists were primarily sensitive to motion artifacts. This conclusion is difficult to come to when interpreting the braindr CNN. Because we employed transfer learning, the features that were extracted were based on the ImageNet classification task, and it is unclear how these features related to MRI-specific artifacts. However, interpretability of deep learning is an ongoing active field of research&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#chakraborty2017interpretability">Chakraborty, 2017</a>)</cite>, and we may be able to fit more interpretable models in the future.</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">Compared to&nbsp; previous efforts to train models to predict quality ratings, such as MRIQC&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, our AUC scores are very high. There are two main reasons for this. First, in the Esteban 2017 study&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, the authors tried to predict the quality of scans from unseen sites, whereas in our study, we combined data across the two &nbsp;sites from which data had been made publicly available at the time we conducted this study. Second, even though our quality ratings on the 3D dataset were continuous scores (ranging from -5 to 5), we only evaluated the performance of our models on data that received an extremely high (4,5) or extremely low score (-4,-5) by the experts. This was because quality control is very subjective, and therefore, there is more variability on images that people are unsure about. An image that was failed with low confidence (-3 to -1) by one researcher could conceivably be passed with low confidence by another researcher (1 to 3). Most importantly, our study had enough data to exclude the images within this range of relative ambiguity in order to train our XGBoost model on both the braindr ratings and the MRIQC features. In studies with less data, such an approach might not be feasible.</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">Another limitation of this method was that our citizen scientists were primarily neuroscientists. The braindr application was advertised on Twitter (<a href="https://www.twitter.com">https://www.twitter.com</a>)&nbsp;by the authors, whose social networks (on this platform) primarily consisted of neuroscientists. As the original tweet travelled outside our social network, we saw more citizen scientists without experience looking at brain images on the platform, but the number of ratings they contributed were not as high as those with neuroscience experience. We also saw that there was an overall tendency for all our users to incorrectly pass images. Future iterations of braindr will include a more informative tutorial and random checks with known images throughout the game to make sure our players are well informed and are performing well throughout the task. In this study, we were able to overcome this limitation because we had enough ratings to train the XGBoost algorithm to preferentially weight some user’s ratings over others.</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="404417" class="ltx_title_subsection container text-justify">Future Directions</h2><div class="container content text-justify mt-3 mb-3">Citizen science platforms like the Zooniverse&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#simpson2014zooniverse">Simpson, 2014</a>)</cite> enable researchers to upload tasks and engage over 1 million citizen scientists. We plan to integrate braindr into a citizen science platform like Zooniverse. This would enable researchers to upload their own data to braindr, and give them access to a diverse group of citizen scientists, rather than only neuroscientists within their social network. We also plan to reuse the braindr interface for more complicated classification tasks in brain imaging. An example could be the classification of ICA components as signal or noise&nbsp;<cite class="ltx_cite raw v1"> (<a href="#griffanti2017hand">Griffanti, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5489418&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, or the evaluation of segmentation algorithms. Finally, incorporating braindr with existing open data initiatives, like OpenNeuro&nbsp;<cite class="ltx_cite raw v1"> (<a href="#gorgolewski2017openneuro">Gorgolewski, 2017</a>)</cite>, or existing neuroimaging platforms like LORIS&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#das2012loris">Das, 2012</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3262165&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> would enable scientists to directly launch braindr tasks from these platforms, which would seamlessly incorporate human in the loop data analysis in neuroimaging research.<b> </b>More generally, the principles described here motivate platforms that integrate citizen science with deep learning for Big Data applications across the sciences. </div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3"></div><h1 data-label="986206" class="ltx_title_section container text-justify">Methods</h1><h2 data-label="500094" class="ltx_title_subsection container text-justify">The Healthy Brain Network Dataset</h2><div class="container content text-justify mt-3 mb-3">The first two releases of the Healthy Brain Network dataset were downloaded from&nbsp;<a href="http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing_neuro.html">http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing_neuro.html</a>&nbsp;. A web application for brain quality control, called Mindcontrol&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Keshavan2017">Keshavan, 2017</a>
            <a href="https://linkinghub.elsevier.com/retrieve/pii/S1053811917302707" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> was hosted at&nbsp;<a href="https://mindcontrol-hbn.herokuapp.com">https://mindcontrol-hbn.herokuapp.com</a>&nbsp;, which enabled users to view and rate 3D MRI images in the browser. There were 724 T1-weighted images. All procedures were approved by the University of Washington Institutional Review Board (IRB). Mindcontrol raters, who were all neuroimaging researchers with substantial experience in similar tasks, provided informed consent, including consent to&nbsp;publicly release these ratings. Mindcontrol raters were asked to pass or fail images after inspecting the full 3D volume, and provide a score of their confidence on a 5 point Likert scale, where 1 was the least confident and 5 was the most confident. Mindcontrol raters received a point for each new volume they rated, and a leaderboard on the homepage displayed rater rankings. The ratings of the top 4 expert raters (including the lead author) were used to create a gold-standard subset of the data.</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="581898" class="ltx_title_subsection container text-justify">Gold-standard Selection</h2><div class="container content text-justify mt-3 mb-3">The gold-standard subset of the data was created by selecting images that were confidently passed or confidently failed (confidence equal or larger than 4) by the 4 expert&nbsp; raters. In order to measure reliability between expert raters, the ratings of the second, third, and fourth expert expert rater were recoded to a scale of -5 to 5 (where -5 is confidently failed, and 5 is confidently passed). An ROC analysis was performed against the binary ratings of the lead author on the commonly rated images, and the area under the curve (AUC) was computed for each pair. An average AUC, weighted by the number of commonly rated images between the pair, was 0.97, showing good agreement between expert raters. The resulting gold-standard dataset consisted of 200 images. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig5">5</a></span> shows example axial slices from the gold-standard dataset.&nbsp;The gold-standard dataset set contains 100 images that were failed by experts, and 100 images that were passed by experts.</div><img class="container mt-3 fig text-center" src="../assets/fig_gold.png" id="fig5"/> 
        <div class="container figcapt"><b>Figure 5</b></div>
        <div class="container text-justify mb-3"><div data-label="169530"><b>Example axial slices from the gold-standard dataset:</b> Passed images show clear contrast between tissue types, and failed images primarily consisted of those with large motion artifacts. We excluded images that failed because of defacing errors from this analysis.</div></div><h2 data-label="855666" class="ltx_title_subsection container text-justify">Data Preparation</h2><div class="container content text-justify mt-3 mb-3">All images were then converted into a set of 2D axial slices using the NiBabel Python library&nbsp;<cite class="ltx_cite raw v1"> (<a href="#matthew_brett_2018_1287921">Brett, 2018</a>
            <a href="https://zenodo.org/record/1287921" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> and uploaded to&nbsp;<a href="https://braindr.us">https://braindr.us</a>. Two images of the 724 were corrupted, so the total image count became 722 images. Five slices, separated by 40 slices, were selected from each brain, where the first slice was one that had over 10,000 non-zero pixels. All slices were padded to 256x256 or 512x512 depending on original image size. One subject (sub-NDARVJ504DAA) had only 4 slices because the last slice did not meet the 10,000 pixel threshold. The total number of slices uploaded to&nbsp;<a href="https://braindr.us">https://braindr.us</a>&nbsp;was 3609.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="712146" class="ltx_title_subsection container text-justify">The braindr web application</h2><div class="container content text-justify mt-3 mb-3">The braindr application was written in Javascript using the Vue.js (<a href="https://vuejs.org">https://vuejs.org</a>) framework. Google Firebase (<a href="https://firebase.google.com/">https://firebase.google.com/</a>) was used for the realtime database. The axial brain slices were hosted on Amazon S3 and served over the Amazon CloudFront content delivery network. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig6">6</a></span>  shows the braindr interface, which presents to the user a 2D slice. &nbsp;On a touchscreen device (tablet or mobile phone), users can swipe right to pass or swipe left to fail the image. On a desktop, a user may click the “pass” or “fail” button or use the right or left arrow keys to classify  the image. The user receives a point for each rating, unless they rate against the majority, where the majority is defined only for images with more than 5 ratings, and where the average rating is below 0.3 or above 0.7. The user receives a notification of the point they earned (or did not earn) for each image after each swipe. All users electronically signed a consent form as approved by the University of Washington IRB. Images were initially served randomly, and then images with fewer ratings were preferentially served.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><div class="container content text-justify mt-3 mb-3">&nbsp;</div><img class="container mt-3 fig text-center" src="../assets/braindrScreenshot.png" id="fig6"/> 
        <div class="container figcapt"><b>Figure 6</b></div>
        <div class="container text-justify mb-3"><div data-label="186072"><b>The braindr web interface:</b> Braindr was hosted at&nbsp;<a href="https://braindr.us">https://braindr.us</a>. Users may click pass or fail buttons, use arrow keys, or swipe on a touchscreen device to rate the image. The top right shows the user's score.&nbsp;</div></div><h2 data-label="214613" class="ltx_title_subsection container text-justify">Braindr data collection</h2><div class="container content text-justify mt-3 mb-3">A total of 261 users submitted over 80,000 ratings. We selected the 25% of the users who rated the largest numbers of the gold-standard slices. This reduced the dataset to 65 users who submitted 68,314 total ratings, 18,940 of which were on the 1000 gold-standard slices. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig7">7</a></span>  shows the distribution of average ratings and the distribution of number of ratings per slice on the gold-standard dataset.</div><div class="container content text-justify mt-3 mb-3"></div><img class="container mt-3 fig text-center" src="../assets/fig_bdr_dist.png" id="fig7"/> 
        <div class="container figcapt"><b>Figure 7</b></div>
        <div class="container text-justify mb-3"><div data-label="358654"><b>Braindr data distributions:</b> Part A shows the distribution of average ratings for each slice on the gold-standard slices. Part B shows the number of ratings per slice, where on average each slice received 20 ratings.</div></div><h2 data-label="188132" class="ltx_title_subsection container text-justify">Rating aggregation with XGBoost</h2><div class="container content text-justify mt-3 mb-3">To aggregate citizen scientist ratings, we weighted citizen scientists based on how consistent their ratings were with the gold-standard. We trained an XGBoost classifier&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#chen2016xgboost">Chen, 2016</a>)</cite> implemented in Python (<a href="http://xgboost.readthedocs.io/en/latest/python/python_intro.html">http://xgboost.readthedocs.io/en/latest/python/python_intro.html</a>) using the cross-validation functions from the scikit-learn Python library&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#pedregosa2011scikit">Pedregosa, 2011</a>)</cite>. We used 600 estimators, and grid searched over a&nbsp; stratified 10-fold cross-validation within the training set to select the optimal maximum depth (2 vs 6) and learning rate (0.01, 0.1). The features of the model were the citizen scientists &nbsp;and each observation was a slice, with the entries in the design matrix set to be the average rating of a specific citizen scientist on a particular slice. We trained the classifier on splits of various sizes of the data to test the dependence on training size (see Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>A). We used the model trained with n=670 to extract the probability scores of the classifier on all 3609 slices in braindr (see Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>B). While equally weighting each citizen scientist’s ratings results in a bimodal distribution with a lower peak that is shifted up from zero (Figure&nbsp;<span class="au-ref raw v1"><a href="#fig7">7</a></span>A), the distribution of probability scores in Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>B more accurately matches our expectations of the data; a bimodal distribution with peaks at 0 and 1. Feature importances were extracted from the model and plotted in Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>C, and plotted against total number of gold-standard image ratings in Figure&nbsp;<span class="au-ref raw v1"><a href="#fig2">2</a></span>D.</div><h2 data-label="682198" class="ltx_title_subsection container text-justify">Deep&nbsp; learning to predict image QC label</h2><div class="container content text-justify mt-3 mb-3">Finally, a deep learning model was trained on the brain slices to predict the XGBoost probability score. All brain slices were resized to 256 by 256 pixels and converted to 3 color channels (RGB) to be compatible with the VGG16 input layer. The data was split into 80%-10%-10% training-validation-test sets. The data was split such that all slices belonging to the same subject were grouped together, so that any individual subject could be only in either training, validation or test.&nbsp;We loaded the VGG16 network that was pretrained with ImageNet weights &nbsp;<cite class="ltx_cite raw v1"> (<a href="#simonyan2014very">Simonyan, 2014</a>)</cite>&nbsp; implemented in Keras&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#chollet2015keras">Chollet, 2015</a>)</cite>, removed the top layer, and ran inference on all the data. The output of the VGG16 inference was then used to train a small sequential neural network consisting of a dense layer with 256 nodes and a rectified linear unit activation function (ReLu), followed by a dropout layer set to drop 50% of the weights to prevent overfitting, and finally a single node output layer with sigmoid activation. The training of the final layer was run for 50 epochs and the best model on the validation set across the 50 epochs was saved. We ran this model 10 separate times, each time with a different random initialization seed, in order to measure the variability of our ROC AUC on the test set.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="304538" class="ltx_title_subsection container text-justify">Training the MRIQC model</h2><div class="container content text-justify mt-3 mb-3">MRIQC was run on all images in the HBN dataset. Rather than using the previously trained MRIQC classifier from Esteban and colleagues&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Esteban2017">Esteban, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite>, the extracted QC features were used to train another XGBoost classifier to predict gold-standard labels. Two thirds of the data was used to train the model, where a 2-fold cross-validation was used to optimize hyper parameters: learning rate = 0.001, 0.01, 0.1, number of estimators = 200, 600, and maximum depth = 2,6,8. An ROC analysis was run, and the computed area under the curve was 0.99.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><h2 data-label="308824" class="ltx_title_subsection container text-justify">Gray matter volume vs age during development</h2><div class="container content text-justify mt-3 mb-3">Finally, to explore the relationship between gray matter volume and age over development as a function of QC threshold, gray matter volume was computed from running the Mindboggle software&nbsp;&nbsp;<cite class="ltx_cite raw v1"> (<a href="#klein2017mindboggling">Klein, 2017</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5322885&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> on the entire dataset. Mindboggle combines the image segmentation output from Freesurfer&nbsp;<cite class="ltx_cite raw v1"> (<a href="#fischl2012freesurfer">Fischl, 2012</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3685476&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> with that of ANTS&nbsp;<cite class="ltx_cite raw v1"> (<a href="#Avants_2011">Avants, 2011</a>
            <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3065962&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>)</cite> to improve the accuracy of segmentation, labeling and volume shape features. Extremely low quality scans did not make it through the entire Mindboggle pipeline, and as a result the dataset size was reduced to 629 for this part of the analysis. The final QC score for the brain volumes was computed by taking the average of the predicted braindr rating from the deep learning model for all five slices. We ran an ordinary least squares (OLS) model on gray matter volume versus age on the data with and without QC thresholding, where the QC threshold was set at 0.7. Figure&nbsp;<span class="au-ref raw v1"><a href="#fig4">4</a></span> shows the result of this analysis, which showed an effect size that nearly doubled and replicated previous findings when QC was performed on the data.&nbsp;</div><div class="container content text-justify mt-3 mb-3"></div><h1 data-label="795074" class="ltx_title_section container text-justify">Acknowledgements</h1><div class="container content text-justify mt-3 mb-3">This research was supported through a grant from the Gordon and Betty Moore Foundation and the Alfred P. Sloan Foundation to the University of Washington eScience Institute. A.K is also supported through a fellowship from the eScience Institute and the University of Washington Institute for Neuroengineering. We thank the NVIDIA corporation for supporting our research through their GPU seed grant. We’d like to acknowledge the following people for fruitful discussions and contributions to the project. Dylan Nielson, Satra Ghosh and Dave Kennedy for the inspiration for braindr. Greg Kiar, for contributing badges to the braindr application, and naming it. Chris Markiewicz, for discussions on application performance, and for application testing in the early stages. Katie Bottenhorn, Dave Kennedy, and Amanda Easson for quality controlling the gold-standard dataset. Jamie Hanson, for sharing the MRIQC metrics. Chris Madan, for application testing and for discussions regarding QC standards. Arno Klein and Lei Ai, for providing us the segmented images from the HBN dataset. Tal Yarkoni and Alejandro de la Vega, for organizing a “code rodeo” for neuroimagers in Austin, TX, where the idea for braindr was born. Finally, we’d like to thank all the citizen scientists who swiped on braindr - we are very grateful for your contributions!</div><div class="container content text-justify mt-3 mb-3"></div><h1 data-label="342587" class="ltx_title_section container text-justify">Code and Data Availability</h1><div class="container content text-justify mt-3 mb-3">The code for the braindr application can be found at&nbsp;<a href="https://doi.org/10.5281/zenodo.1208140">https://doi.org/10.5281/zenodo.1208140</a>. The brain slice data is hosted at &nbsp;<a href="https://osf.io/j5d4y/">https://osf.io/j5d4y/</a>&nbsp;. The code for the analysis for this project, including all figures and the source code for the interactive version of this manuscript, can be found at&nbsp;<a href="https://github.com/akeshavan/braindr-results">https://github.com/akeshavan/braindr-results</a>&nbsp;and&nbsp;<a href="https://github.com/akeshavan/braindr-analysis" target="_blank">https://github.com/akeshavan/braindr-analysis</a>.</div><h2 class="container text-justify">References</h2><div class="container references">
<div id="Van_Horn_2013" class="bib text-left mt-2 mb-2">Van Horn, John D.  and Toga, Arthur W.  "Human neuroimaging as a \textquotedblleftBig Data\textquotedblright science" 
    <i>Brain Imaging and Behavior</i> 8.2 (2013):323--331 <a href="https://doi.org/10.1007%2Fs11682-013-9255-y">link</a>
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3983169&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Fan_2014" class="bib text-left mt-2 mb-2">Fan, Jianqing el al., "Challenges of Big Data analysis" 
    <i>National Science Review</i> 1.2 (2014):293--314 <a href="https://doi.org/10.1093%2Fnsr%2Fnwt032">link</a>
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4236847&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Ferguson_2014" class="bib text-left mt-2 mb-2">Ferguson, Adam R el al., "Big data from small data: data-sharing in the \textquotesinglelong tail\textquotesingle of neuroscience" 
    <i>Nature Neuroscience</i> 17.11 (2014):1442--1447 <a href="https://doi.org/10.1038%2Fnn.3838">link</a>
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4728080&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Poldrack_2014" class="bib text-left mt-2 mb-2">Poldrack, Russell A  and Gorgolewski, Krzysztof J  "Making big data open: data sharing in neuroimaging" 
    <i>Nature Neuroscience</i> 17.11 (2014):1510--1517 <a href="https://doi.org/10.1038%2Fnn.3818">link</a>
    <a href="http://pubman.mpdl.mpg.de/pubman/faces/viewItemOverviewPage.jsp?itemId=escidoc:2075310" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Jordan_2017" class="bib text-left mt-2 mb-2">Jordan, Kesshi M. el al., "Cluster Confidence Index: A Streamline-Wise Pathway Reproducibility Metric for Diffusion-Weighted MRI Tractography" 
    <i>Journal of Neuroimaging</i> 28.1 (2017):64--69 <a href="https://doi.org/10.1111%2Fjon.12467">link</a>
    <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/jon.12467" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="fischl2012freesurfer" class="bib text-left mt-2 mb-2">Fischl, Bruce "FreeSurfer" 
    <i>Neuroimage</i> 62.2 (2012):774--781 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3685476&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Patenaude_2011" class="bib text-left mt-2 mb-2">Patenaude, Brian el al., "A Bayesian model of shape and appearance for subcortical brain segmentation" 
    <i>NeuroImage</i> 56.3 (2011):907--922 <a href="https://doi.org/10.1016%2Fj.neuroimage.2011.02.046">link</a>
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3417233&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Avants_2011" class="bib text-left mt-2 mb-2">Avants, Brian B. el al., "A reproducible evaluation of ANTs similarity metric performance in brain image registration" 
    <i>NeuroImage</i> 54.3 (2011):2033--2044 <a href="https://doi.org/10.1016%2Fj.neuroimage.2010.09.025">link</a>
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3065962&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Ashburner_2005" class="bib text-left mt-2 mb-2">Ashburner, John  and Friston, Karl J.  "Unified segmentation" 
    <i>NeuroImage</i> 26.3 (2005):839--851 <a href="https://doi.org/10.1016%2Fj.neuroimage.2005.02.018">link</a>
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4809890&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="CATANI_2008" class="bib text-left mt-2 mb-2">Catani, M  and Thiebautdeschotten, M  "A diffusion tensor imaging tractography atlas for virtual in vivo dissections" 
    <i>Cortex</i> 44.8 (2008):1105--1132 <a href="https://doi.org/10.1016%2Fj.cortex.2008.05.004">link</a>
    <a href="http://www.hal.inserm.fr/inserm-00322865" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Jordan_2017a" class="bib text-left mt-2 mb-2">Jordan, Kesshi Marin el al., "Investigating The Functional Consequence Of White Matter Damage: An Automatic Pipeline To Create Longitudinal Disconnection Tractograms" 
    <i></i>(2017): <a href="https://doi.org/10.1101%2F140137">link</a>
    <a href="https://www.biorxiv.org/content/biorxiv/early/2017/05/20/140137.full.pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="ronneberger2015u" class="bib text-left mt-2 mb-2">Ronneberger, Olaf el al., "U-net: Convolutional networks for biomedical image segmentation" 
    <i></i>(2015):234--241 
    
    </div><div id="Lee_2017" class="bib text-left mt-2 mb-2">Lee, Cecilia S. el al., "Deep-learning based automated segmentation of macular edema in optical coherence tomography" 
    <i>Biomedical Optics Express</i> 8.7 (2017):3440 <a href="https://doi.org/10.1364%2Fboe.8.003440">link</a>
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5508840&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Dalm__2017" class="bib text-left mt-2 mb-2">Dalmış, Mehmet Ufuk el al., "Using deep learning to segment breast and fibroglandular tissue in MRI volumes" 
    <i>Medical Physics</i> 44.2 (2017):533--546 <a href="https://doi.org/10.1002%2Fmp.12079">link</a>
    <a href="http://repository.ubn.ru.nl/handle/2066/173062" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="cciccek20163d" class="bib text-left mt-2 mb-2">Çiçek, Özgün el al., "3D U-Net: learning dense volumetric segmentation from sparse annotation" 
    <i></i>(2016):424--432 
    
    </div><div id="van2015transfer" class="bib text-left mt-2 mb-2">Annegreet, Van Opbroek el al., "Transfer learning improves supervised image segmentation across imaging protocols" 
    <i>IEEE transactions on medical imaging</i> 34.5 (2015):1018--1030 
    <a href="https://curis.ku.dk/portal/da/publications/transfer-learning-improves-supervised-image-segmentation-across-imaging-protocols(07623de4-ed6c-4d29-867a-ea9a1a639dba).html" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="ahmed2008training" class="bib text-left mt-2 mb-2">Ahmed, Amr el al., "Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks" 
    <i></i>(2008):69--82 
    
    </div><div id="pan2010survey" class="bib text-left mt-2 mb-2">Jialin, Sinno Pan  and Yang, Qiang  "A survey on transfer learning" 
    <i>IEEE Transactions on knowledge and data engineering</i> 22.10 (2010):1345--1359 
    <a href="https://ieeexplore.ieee.org/document/5288526/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="heuer2016open" class="bib text-left mt-2 mb-2">Heuer, Katja el al., "Open neuroimaging laboratory" 
    <i>Research Ideas and Outcomes</i> 2. (2016):e9113 
    <a href="https://riojournal.com/articles.php?journal_name=rio&id=9113" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Keshavan2017" class="bib text-left mt-2 mb-2">Keshavan, Anisha el al., "Mindcontrol: A web application for brain segmentation quality control" 
    <i>NeuroImage</i>(2017): 
    <a href="https://linkinghub.elsevier.com/retrieve/pii/S1053811917302707" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="alexander2017open" class="bib text-left mt-2 mb-2">Alexander, Lindsay M el al., "An open resource for transdiagnostic research in pediatric mental health and learning disorders" 
    <i>Scientific data</i> 4. (2017):170181 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5735921&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="ducharme2016trajectories" class="bib text-left mt-2 mb-2">Ducharme, Simon el al., "Trajectories of cortical thickness maturation in normal brain development—The importance of quality control procedures" 
    <i>Neuroimage</i> 125. (2016):267--279 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4691414&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Lebel2011" class="bib text-left mt-2 mb-2">Lebel, Catherine  and Beaulieu, Christian  "Longitudinal development of human brain wiring continues from childhood into adulthood" 
    <i>Journal of Neuroscience</i> 31.30 (2011):10937--10947 
    <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004821/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="kim2014space" class="bib text-left mt-2 mb-2">Kim, Jinseop S el al., "Space--time wiring specificity supports direction selectivity in the retina" 
    <i>Nature</i> 509.7500 (2014):331 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4074887&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="marx2013neuroscience" class="bib text-left mt-2 mb-2">Marx, Vivien "Neuroscience waves to the crowd" 
    <i></i>(2013): 
    
    </div><div id="roskams2016power" class="bib text-left mt-2 mb-2">Roskams, Jane  and Popović, Zoran  "Power to the people: Addressing big data challenges in neuroscience by creating a new cadre of citizen neuroscientists" 
    <i>Neuron</i> 92.3 (2016):658--664 
    <a href="https://linkinghub.elsevier.com/retrieve/pii/S0896627316001689" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="mortamet2009automatic" class="bib text-left mt-2 mb-2">Mortamet, Bénédicte el al., "Automatic quality assessment in structural brain magnetic resonance imaging" 
    <i>Magnetic resonance in medicine</i> 62.2 (2009):365--372 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2780021&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="shehzadpreprocessed" class="bib text-left mt-2 mb-2">Shehzad, Zarrar el al., "The Preprocessed Connectomes Project Quality Assessment Protocol: A resource for measuring the quality of MRI data" 
    <i>Frontiers in Neuroscience</i> .47 (2015): <a href="http://www.frontiersin.org/10.3389/conf.fnins.2015.91.00047/event_abstract">link</a>
    <a href="https://www.frontiersin.org/10.3389/conf.fnins.2015.91.00047/event_abstract" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="chakraborty2017interpretability" class="bib text-left mt-2 mb-2">Chakraborty, Supriyo el al., "Interpretability of deep learning models: a survey of results" 
    <i></i>(2017): 
    
    </div><div id="simpson2014zooniverse" class="bib text-left mt-2 mb-2">Simpson, Robert el al., "Zooniverse: observing the world's largest citizen science platform" 
    <i></i>(2014):1049--1054 
    
    </div><div id="griffanti2017hand" class="bib text-left mt-2 mb-2">Griffanti, Ludovica el al., "Hand classification of fMRI ICA noise components" 
    <i>Neuroimage</i> 154. (2017):188--205 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5489418&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="gorgolewski2017openneuro" class="bib text-left mt-2 mb-2">Gorgolewski, K el al., "OpenNeuro—a free online platform for sharing and analysis of neuroimaging data" 
    <i>Organization for Human Brain Mapping. Vancouver, Canada</i>(2017):1677 
    
    </div><div id="das2012loris" class="bib text-left mt-2 mb-2">Das, Samir el al., "LORIS: a web-based data management system for multi-center studies" 
    <i>Frontiers in neuroinformatics</i> 5. (2012):37 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3262165&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="pedregosa2011scikit" class="bib text-left mt-2 mb-2">Pedregosa, Fabian el al., "Scikit-learn: Machine learning in Python" 
    <i>Journal of machine learning research</i> 12.Oct (2011):2825--2830 
    
    </div><div id="simonyan2014very" class="bib text-left mt-2 mb-2">Simonyan, Karen  and Zisserman, Andrew  "Very deep convolutional networks for large-scale image recognition" 
    <i>arXiv preprint arXiv:1409.1556</i>(2014): <a href="https://arxiv.org/abs/1409.1556e">link</a>
    
    </div><div id="chollet2015keras" class="bib text-left mt-2 mb-2">Chollet, François "Keras" 
    <i></i>(2015): 
    
    </div><div id="klein2017mindboggling" class="bib text-left mt-2 mb-2">Klein, Arno el al., "Mindboggling morphometry of human brains" 
    <i>PLoS computational biology</i> 13.2 (2017):e1005350 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5322885&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Chen2016" class="bib text-left mt-2 mb-2">Chen, Tianqi  and Guestrin, Carlos  "Xgboost: A scalable tree boosting system" 
    <i></i>(2016):785--794 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5855321&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Esteban2017" class="bib text-left mt-2 mb-2">Esteban, Oscar el al., "MRIQC: Advancing the automatic prediction of image quality in MRI from unseen sites" 
    <i>PloS one</i> 12.9 (2017):e0184661 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5612458&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Benson2014" class="bib text-left mt-2 mb-2">Benson, NC el al., "Correction of distortion in flattened representations of the cortical surface allows prediction of V1-V3 functional organization from anatomy." 
    <i>PLoS Comput Biol</i> 10. (2014):e1003538 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3967932&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Benson2012" class="bib text-left mt-2 mb-2">Benson, NC el al., "The retinotopic organization of striate cortex is well predicted by surface topology." 
    <i>Curr Biol</i> 22. (2012):2081-5 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3494819&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Winawer2017" class="bib text-left mt-2 mb-2">Winawer, J  and Witthoft, N  "Identification of the ventral occipital visual field maps in the human brain." 
    <i>F1000Res</i> 6. (2017):1526 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5687318&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="Wandell2011" class="bib text-left mt-2 mb-2">Wandell, BA  and Winawer, J  "Imaging retinotopic maps in the human brain." 
    <i>Vision Res</i> 51. (2011):718-37 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC3030662&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="ILSVRC15" class="bib text-left mt-2 mb-2">Russakovsky, Olga el al., "ImageNet Large Scale Visual Recognition Challenge" 
    <i>International Journal of Computer Vision (IJCV)</i> 115.3 (2015):211-252 
    <a href="http://dspace.mit.edu/bitstream/handle/1721.1/104944/11263_2015_Article_816.pdf?sequence=1" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="saito2015precision" class="bib text-left mt-2 mb-2">Saito, Takaya  and Rehmsmeier, Marc  "The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets" 
    <i>PloS one</i> 10.3 (2015):e0118432 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4349800&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="krizhevsky2012imagenet" class="bib text-left mt-2 mb-2">Krizhevsky, Alex el al., "Imagenet classification with deep convolutional neural networks" 
    <i></i>(2012):1097--1105 
    <a href="http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="girshick2014rich" class="bib text-left mt-2 mb-2">Girshick, Ross el al., "Rich feature hierarchies for accurate object detection and semantic segmentation" 
    <i></i>(2014):580--587 <a href="https://arxiv.org/abs/1311.2524">link</a>
    <a href="http://www.computeroptics.ru/jour/article/view/439" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="long2015fully" class="bib text-left mt-2 mb-2">Long, Jonathan el al., "Fully convolutional networks for semantic segmentation" 
    <i></i>(2015):3431--3440 
    <a href="https://ieeexplore.ieee.org/document/7478072/" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="esteva2017dermatologist" class="bib text-left mt-2 mb-2">Esteva, Andre el al., "Dermatologist-level classification of skin cancer with deep neural networks" 
    <i>Nature</i> 542.7639 (2017):115 
    
    </div><div id="sahiner1996classification" class="bib text-left mt-2 mb-2">Sahiner, Berkman el al., "Classification of mass and normal breast tissue: a convolution neural network classifier with spatial domain and texture images" 
    <i>IEEE transactions on Medical Imaging</i> 15.5 (1996):598--610 
    
    </div><div id="lee2017deep" class="bib text-left mt-2 mb-2">Lee, Cecilia S el al., "Deep learning is effective for classifying normal versus age-related macular degeneration OCT images" 
    <i>Ophthalmology Retina</i> 1.4 (2017):322--327 
    <a href="https://linkinghub.elsevier.com/retrieve/pii/S2468653016301749" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="lee2017deepa" class="bib text-left mt-2 mb-2">Lee, Cecilia S el al., "Deep-learning based, automated segmentation of macular edema in optical coherence tomography" 
    <i>Biomedical optics express</i> 8.7 (2017):3440--3448 
    <a href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC5508840&blobtype=pdf" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="chen2016xgboost" class="bib text-left mt-2 mb-2">Chen, Tianqi  and Guestrin, Carlos  "Xgboost: A scalable tree boosting system" 
    <i></i>(2016):785--794 
    
    </div><div id="glasser2016human" class="bib text-left mt-2 mb-2">Glasser, Matthew F el al., "The human connectome project's neuroimaging approach" 
    <i>Nature Neuroscience</i> 19.9 (2016):1175 
    
    </div><div id="gorgolewski2016brain" class="bib text-left mt-2 mb-2">Gorgolewski, Krzysztof J el al., "The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments" 
    <i>Scientific Data</i> 3. (2016):160044 
    
    </div><div id="gorgolewski2017bids" class="bib text-left mt-2 mb-2">Gorgolewski, Krzysztof J el al., "BIDS apps: Improving ease of use, accessibility, and reproducibility of neuroimaging data analysis methods" 
    <i>PLoS computational biology</i> 13.3 (2017):e1005209 
    
    </div><div id="casey2018adolescent" class="bib text-left mt-2 mb-2">Casey, BJ el al., "The adolescent brain cognitive development (ABCD) study: imaging acquisition across 21 sites" 
    <i>Developmental cognitive neuroscience</i>(2018): 
    
    </div><div id="cheng2015break" class="bib text-left mt-2 mb-2">Cheng, Justin el al., "Break it down: A comparison of macro-and microtasks" 
    <i></i>(2015):4061--4064 
    
    </div><div id="yeatman2012tract" class="bib text-left mt-2 mb-2">Yeatman, Jason D el al., "Tract profiles of white matter properties: automating fiber-tract quantification" 
    <i>PloS one</i> 7.11 (2012):e49790 
    <a href="http://europepmc.org/articles/PMC3498174?pdf=render" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div><div id="matthew_brett_2018_1287921" class="bib text-left mt-2 mb-2">Brett, Matthew el al., "nipy/nibabel: 2.3.0" 
    <i></i>(2018): <a href="https://doi.org/10.5281/zenodo.1287921">link</a>
    <a href="https://zenodo.org/record/1287921" v-b-tooltip.hover title="View Open Access Article"><img class="oabutton" src="../assets/icon_OAB.png"/></a>
    </div></div>

  </div>

</template>



<style>

.ltx_title_section {

  padding-left: 15px;

  margin-left: auto;

  margin-right: auto;

  margin-top: 1em;

  font-size: 2rem;

}



cite.ltx_cite.raw.v1 {

    color: #378eff;

}



img.oabutton {

  height: 1rem;

}



.affiliations {

  font-size: small;

}



.fig {

  width: 75%;

  max-width: 600px;

  max-height: 25%;

}



.abstract {

  font-size: 0.95rem;

}



.ltx_title_subsection {

  padding-left: 15px;

  margin-left: auto;

  margin-right: auto;

  margin-top: 0.5em;

  font-size: 1.5rem;

}

</style>



<script>

import HelloWorld from './HelloWorld';



export default {

  name: 'main0',

  components: { HelloWorld },

  data() {

    return {

    };

  },

};

</script>
